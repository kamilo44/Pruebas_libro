
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Optimización &#8212; Prueba_1</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Probabilidad" href="Prob_Conceptos_Basicos.html" />
    <link rel="prev" title="Tensores" href="Intro_Tensores_II.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Prueba_1</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Buscar este libro ..." aria-label="Buscar este libro ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro_Tensores_II.html">
   <span style="color:#F72585">
    <center>
     Tensores
    </center>
   </span>
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   <span style="color:#F72585">
    <center>
     Optimización
    </center>
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Prob_Conceptos_Basicos.html">
   <span style="color:#F72585">
    <center>
     Probabilidad
    </center>
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Prob_Variables_Aleatorias.html">
   <span style="color:#F72585">
    <center>
     Variables Aleatorias
    </center>
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ti_Teoria_Informacion.html">
   <span style="color:#F72585">
    <center>
     Teoría de la Información
    </center>
   </span>
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navegación de palanca" aria-controls="site-navigation"
                title="Navegación de palanca" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Descarga esta pagina"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/optimizacion_sgd.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Descargar archivo fuente" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Imprimir en PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kamilo44/Prueba_1"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Repositorio de origen"><i
                    class="fab fa-github"></i>repositorio</button></a>
        <a class="issues-button"
            href="https://github.com/kamilo44/Prueba_1/issues/new?title=Issue%20on%20page%20%2Foptimizacion_sgd.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Abrir un problema"><i class="fas fa-lightbulb"></i>Tema abierto</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Modo de pantalla completa"
        title="Modo de pantalla completa"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/kamilo44/Prueba_1/main?urlpath=tree/docs/optimizacion_sgd.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Lanzamiento Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/kamilo44/Prueba_1/blob/main/docs/optimizacion_sgd.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Lanzamiento Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Lanzamiento Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contenido
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-referencias-span">
   <span style="color:#4361EE">
    Referencias
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-introduccion-span">
   <span style="color:#4361EE">
    Introducción
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-metodos-de-optimizacion-basados-en-el-gradiente-span">
   <span style="color:#4361EE">
    Métodos de optimización basados en el gradiente
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-gradiente-descendiente-en-lote-span">
   <span style="color:#4361EE">
    Gradiente descendiente en lote
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-gradiente-descendiente-estocastico-span">
   <span style="color:#4361EE">
    Gradiente descendiente estocástico
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-gradiente-descendiente-estocastico-por-mini-lotes-span">
   <span style="color:#4361EE">
    Gradiente descendiente estocástico por mini-lotes
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#span-style-color-4cc9f0-discusion-span">
     <span style="color:#4CC9F0">
      Discusión
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-metodo-del-momento-span">
   <span style="color:#4361EE">
    Método del momento
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-rmsprop-span">
   <span style="color:#4361EE">
    RMSprop
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-algoritmo-adam-span">
   <span style="color:#4361EE">
    Algoritmo Adam
   </span>
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1><span style="color:#F72585"><center>Optimización</center></span></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contenido </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-referencias-span">
   <span style="color:#4361EE">
    Referencias
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-introduccion-span">
   <span style="color:#4361EE">
    Introducción
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-metodos-de-optimizacion-basados-en-el-gradiente-span">
   <span style="color:#4361EE">
    Métodos de optimización basados en el gradiente
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-gradiente-descendiente-en-lote-span">
   <span style="color:#4361EE">
    Gradiente descendiente en lote
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-gradiente-descendiente-estocastico-span">
   <span style="color:#4361EE">
    Gradiente descendiente estocástico
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-gradiente-descendiente-estocastico-por-mini-lotes-span">
   <span style="color:#4361EE">
    Gradiente descendiente estocástico por mini-lotes
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#span-style-color-4cc9f0-discusion-span">
     <span style="color:#4CC9F0">
      Discusión
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-metodo-del-momento-span">
   <span style="color:#4361EE">
    Método del momento
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-rmsprop-span">
   <span style="color:#4361EE">
    RMSprop
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-algoritmo-adam-span">
   <span style="color:#4361EE">
    Algoritmo Adam
   </span>
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="span-style-color-f72585-center-optimizacion-center-span">
<h1><span style="color:#F72585"><center>Optimización</center></span><a class="headerlink" href="#span-style-color-f72585-center-optimizacion-center-span" title="Permalink to this headline">¶</a></h1>
<center>Gradiente Descendiente Estocástico</center><p><img alt="" src="../Imagenes/512px-3d-gradient-cos.svg.png" /></p>
<p>Fuente: <a class="reference external" href="https://commons.wikimedia.org/wiki/File:3d-gradient-cos.svg">Wikipedia</a></p>
<div class="section" id="span-style-color-4361ee-referencias-span">
<h2><span style="color:#4361EE">Referencias</span><a class="headerlink" href="#span-style-color-4361ee-referencias-span" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p><a class="reference external" href="https://github.com/AprendizajeProfundo/Diplomado">Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021</a></p></li>
<li><p><a class="reference external" href="https://github.com/AprendizajeProfundo/Diplomado-Avanzado">Alvaro Montenegro, Daniel Montenegro y Oleg Jarma, Inteligencia Artificial y Aprendizaje Profundo Avanzado, 2022</a></p></li>
</ol>
</div>
<div class="section" id="span-style-color-4361ee-introduccion-span">
<h2><span style="color:#4361EE">Introducción</span><a class="headerlink" href="#span-style-color-4361ee-introduccion-span" title="Permalink to this headline">¶</a></h2>
<p>La mayoría de los algoritmos de aprendizaje profundo implican optimización de algún tipo. La optimización se refiere a la tarea de minimizar o maximizar alguna función <span class="math notranslate nohighlight">\( f (x) \)</span> alterando <span class="math notranslate nohighlight">\( x \)</span>. Por lo general, expresamos la mayoría de los problemas de optimización en aprendizaje profundo en términos de minimizar una función <span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
<p><strong>Entenderemos la frase  minimizar una función <span class="math notranslate nohighlight">\(f(x)\)</span> como un procedimiento para encontrar valor <span class="math notranslate nohighlight">\(x^*\)</span> de tal manera que <span class="math notranslate nohighlight">\( f (x^*) \)</span> tenga el menor valor posible.</strong></p>
<p>Los matemáticos escriben esta frase en símbolos de la siguiente manera:</p>
<div class="math notranslate nohighlight">
\[
x^* = \underset{x}{\operatorname{argmin}}  f(x).
\]</div>
<p>La función que queremos minimizar  se llama función o criterio <strong>objetivo</strong>. Cuando estamos minimizando, también podemos llamarla función de costo, <strong>función de pérdida</strong> o función de error.</p>
<p>La búsqueda de un mínimo global puede ser una tarea muy dura en aprendizaje de máquinas si se tiene en cuenta que las funciones tienen muchas variables y consecuencia se tienen muchas dimensiones, por lo que no podemos <em>verlas</em>. En la siguiente imagen la función tiene dos variables <strong>(features)</strong>. En aprendizaje de máquinas se pueden tener cientos  miles y hasta más variables (features). La siguiente imagen muestra una función con varios máximos y varios mínimos.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">LinearLocator</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;projection&quot;</span><span class="p">:</span> <span class="s2">&quot;3d&quot;</span><span class="p">},</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>


<span class="c1"># Make data.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>

<span class="c1"># Plot the surface.</span>
<span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span>
                       <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Customize the z axis.</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">zaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">LinearLocator</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="c1"># A StrMethodFormatter is used automatically</span>
<span class="n">ax</span><span class="o">.</span><span class="n">zaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{x:.02f}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Add a color bar which maps values to colors.</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">surf</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/optimizacion_sgd_8_0.png" src="_images/optimizacion_sgd_8_0.png" />
</div>
</div>
</div>
<div class="section" id="span-style-color-4361ee-metodos-de-optimizacion-basados-en-el-gradiente-span">
<h2><span style="color:#4361EE">Métodos de optimización basados en el gradiente</span><a class="headerlink" href="#span-style-color-4361ee-metodos-de-optimizacion-basados-en-el-gradiente-span" title="Permalink to this headline">¶</a></h2>
<p>En esta lección vamos a concentrarnos en las técnicas más modernas de optimización desarrolladas para el hacer posible el aprendizaje de máquinas. En este contexto se tiene que:</p>
<p><strong><center>El problema de entrenar una máquina es un problema de optimización.</center></strong></p>
<p>A menudo minimizamos las funciones que tienen múltiples entradas: <span class="math notranslate nohighlight">\( f: \mathbb{R}^n \to \mathbb {R} \)</span>.</p>
<p>Para que el concepto de “minimización” tenga sentido, debe haber una sola salida (función escalar).  Para funciones con múltiples entradas, se hace uso del concepto de derivadas parciales. La derivada parcial <span class="math notranslate nohighlight">\(\frac{\partial}{\partial x_i} f(x)\)</span>  mide como cambia (la velocidad a la que cambia) <span class="math notranslate nohighlight">\(f\)</span>  cuando la variable <span class="math notranslate nohighlight">\(x_i\)</span> crece o decrece desde el punto  <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>El gradiente generaliza la noción de derivada al caso en que la derivada es con respecto a una dirección en el espacio. El gradiente de <span class="math notranslate nohighlight">\( f \)</span>, denotado <span class="math notranslate nohighlight">\( \nabla_xf (x) \)</span>,  es el vector que contiene todas las derivadas parciales. El elemento <span class="math notranslate nohighlight">\( i \)</span> del gradiente es la derivada parcial de <span class="math notranslate nohighlight">\( f \)</span> con respecto a <span class="math notranslate nohighlight">\( x_i \)</span>.</p>
<p>En múltiples dimensiones, los <em>puntos críticos</em> son puntos donde cada elemento del gradiente es igual a cero. Por otro lado, se puede verificar que el gradiente <span class="math notranslate nohighlight">\( \nabla_xf (x) \)</span>  es ese  vector que apunta en la dirección en la cual la función <span class="math notranslate nohighlight">\(f\)</span> crece más rápidamente partiendo precisamente del punto <span class="math notranslate nohighlight">\(x\)</span>. En consecuencia, <span class="math notranslate nohighlight">\( -\nabla_xf (x) \)</span> apunta en la dirección contraria, es decir en la dirección hacia la cual la función decrece más rápido, desde el punto <span class="math notranslate nohighlight">\(x\)</span>. Esta es la clave de los métodos de optimización basados en el gradiente.</p>
<p>La siguiente imagen ilustra el gradiente proyectado en el plano <span class="math notranslate nohighlight">\(xy\)</span> de la función <span class="math notranslate nohighlight">\(f(x,y)= -(\cos x^2 + \sin x^2)^2\)</span>.</p>
<p><img alt="" src="../Imagenes/512px-3d-gradient-cos.svg.png" /></p>
<p>Gradientes de la función <span class="math notranslate nohighlight">\(f(x,y)= -(\cos x^2 + \sin x^2)^2\)</span> proyectados en el plano <span class="math notranslate nohighlight">\(xy\)</span></p>
<p>Fuente: <a class="reference external" href="https://commons.wikimedia.org/wiki/File:3d-gradient-cos.svg">Wikipedia</a></p>
<p>El término <strong>gradiente descendiente</strong> indica que se usará <span class="math notranslate nohighlight">\( -\nabla_xf (x) \)</span> para moverse a un siguiente punto en busca de un mínimo local. El método general se escribe como:</p>
<div class="math notranslate nohighlight">
\[
x^{(k+1)} = x^{(k)} − \eta_{k} \nabla_x f(x^{(k)})
\]</div>
<p>Los  valores  <span class="math notranslate nohighlight">\(\eta_k\)</span> se denominan genéricamente  <strong>tasa de aprendizaje</strong>. La razón de incorporar la tasa de aprendizaje es controlar el tamaño de paso. Si no hace esta corrección podemos alejarnos en lugar de acercarnos al mínimo que se está buscando.</p>
<p><img alt="Gradiente descendiente" src="../Imagenes/350px-Gradient_descent.svg.png" /></p>
<p>Ilustración usando curvas de nivel de como ocurren las iteraciones en el método del gradiente descendiente.</p>
<p>Fuente: <a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent">Wikipedia</a></p>
</div>
<div class="section" id="span-style-color-4361ee-gradiente-descendiente-en-lote-span">
<h2><span style="color:#4361EE">Gradiente descendiente en lote</span><a class="headerlink" href="#span-style-color-4361ee-gradiente-descendiente-en-lote-span" title="Permalink to this headline">¶</a></h2>
<p>En el método de gradiente  descendiente vainilla (vainilla se refiere al ejemplo básico), también conocido como descenso de gradiente por lotes, calcula el gradiente de la función de pérdida con respecto a los parámetros <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> para el <strong>conjunto de datos de entrenamiento completo</strong> <span class="math notranslate nohighlight">\((\mathbf{x}_{train},\mathbf{y}_{train})\)</span>. Si <span class="math notranslate nohighlight">\(\mathfrak{L}\)</span> es la función de pérdida del problema, entonces se tiene que</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}_{k+1} =  \boldsymbol{\theta}_k - \eta_k \nabla_{\boldsymbol{\theta}} \mathfrak{L}(\mathbf{x}_{train},\mathbf{y}_{train},\boldsymbol{\theta}_k),
\]</div>
<p>El principal problema a resolver con los métodos de gradiente descendiente es cómo definir y actualizar en cada paso la tasa de aprendizaje <span class="math notranslate nohighlight">\(\eta_k \)</span>. Un fragmento de código, en el cual se actualiza la tasa de aprendizaje podría lucir como sigue. Supongamos que al comenzar <span class="math notranslate nohighlight">\(0&lt;\eta_0&lt;1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gd</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_func</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">-=</span>  <span class="n">eta</span> <span class="o">*</span> <span class="n">gradient</span>
        <span class="n">eta</span>   <span class="o">*=</span> <span class="n">eta</span>
    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">gradient</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="span-style-color-4361ee-gradiente-descendiente-estocastico-span">
<h2><span style="color:#4361EE">Gradiente descendiente estocástico</span><a class="headerlink" href="#span-style-color-4361ee-gradiente-descendiente-estocastico-span" title="Permalink to this headline">¶</a></h2>
<p>El descenso de gradiente estocástico (SGD), por el contrario, realiza una actualización de parámetros para cada ejemplo de entrenamiento <span class="math notranslate nohighlight">\(x_{train}^{(i)} \)</span> y etiqueta <span class="math notranslate nohighlight">\( y_{train}^ {(i)} \)</span>, <strong>seleccionados al azar en cada época</strong>.</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}_{k+1} =  \boldsymbol{\theta}_k - \eta_k \nabla_{\boldsymbol{\theta}} \mathfrak{L}({x}_{train}^{(i)},{y}_{train}^{(i)},\boldsymbol{\theta}_k),
\]</div>
<p>En el artículo original de <a class="reference external" href="https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586">Robbins and Monro (1951)</a> <span class="math notranslate nohighlight">\(\eta\)</span> cambia en cada iteración como acabamos de mostrar y se asume que  <span class="math notranslate nohighlight">\(\{\eta_k\}\)</span> es una sucesión tal que <span class="math notranslate nohighlight">\(\sum_k \eta_k = \infty\)</span>, and <span class="math notranslate nohighlight">\(\sum_k \eta_k^2 &lt; \infty\)</span>. Por ejemplo, se puede escoger <span class="math notranslate nohighlight">\(\eta_k = 1/k\)</span>. Robbins y Monro demostraron que bajo condiciones muy generales este algoritmo converge a la solución de problema, con probabilidad 1.</p>
<p>Un fragmento de código del algoritmo de Robbins and Monro podría lucir como sigue.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">data_train</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span> <span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">example</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_func</span><span class="p">,</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span> <span class="p">)</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gradient</span>
            <span class="n">eta</span> <span class="o">*=</span> <span class="n">eta</span>
    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">gradient</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="span-style-color-4361ee-gradiente-descendiente-estocastico-por-mini-lotes-span">
<h2><span style="color:#4361EE">Gradiente descendiente estocástico por mini-lotes</span><a class="headerlink" href="#span-style-color-4361ee-gradiente-descendiente-estocastico-por-mini-lotes-span" title="Permalink to this headline">¶</a></h2>
<p>El descenso de gradiente por mini-lotes finalmente toma lo mejor de los dos mundos anteriores y realiza una actualización para cada mini-lote de <span class="math notranslate nohighlight">\(n\)</span> ejemplos de entrenamiento:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}_{k+1} =  \boldsymbol{\theta}_k - \eta_k \nabla_{\boldsymbol{\theta}} \mathfrak{L}(\mathbf{x}_{train}^{(i:i+n)},\mathbf{y}_{train}^{(i:i+n)},\boldsymbol{\theta}_k),
\]</div>
<p>Desde este punto de la lección, asumiremos que <strong>tomamos mini-lotes</strong>, por lo que omitimos súper-índices en los datos <span class="math notranslate nohighlight">\((\mathbf{x}_{train}^{(i:i+n)},\mathbf{y}_{train}^{(i:i+n)})\)</span> en todas las expresiones.</p>
<p>Un fragmento de código para este método podría lucir como sigue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sgd_mini_batch</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">data_train</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span> <span class="p">(</span><span class="n">data_train</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">get_batches</span><span class="p">(</span><span class="n">data_train</span> <span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_func</span><span class="p">,</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span> <span class="p">)</span>
            <span class="n">theta</span> <span class="o">-=</span>  <span class="n">eta</span> <span class="o">*</span> <span class="n">gradient</span>
            <span class="n">eta</span> <span class="o">*=</span> <span class="n">eta</span>
    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">gradient</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-nota admonition">
<p class="admonition-title">Nota</p>
<p>El tamaño de los mini-lotes depende del problema y puede ser 32, 64, 128, etc. En el ejemplo,  <em>get_batches()</em> es una función generadora que va entregando lotes de datos a la medida que el algoritmo los requiere. Para las TPU se esperan mini-lotes de tamaño que sea múltiplo de 128.</p>
</div>
<div class="section" id="span-style-color-4cc9f0-discusion-span">
<h3><span style="color:#4CC9F0">Discusión</span><a class="headerlink" href="#span-style-color-4cc9f0-discusion-span" title="Permalink to this headline">¶</a></h3>
<p>El método vainilla del descenso de gradiente  no garantiza una buena convergencia, y ofrece algunos desafíos que deben abordarse:</p>
<ol class="simple">
<li><p>Elegir un ritmo de aprendizaje adecuado puede resultar complicado. Una tasa de aprendizaje demasiado pequeña conduce a una convergencia dolorosamente lenta, mientras que una tasa de aprendizaje demasiado grande puede dificultar la convergencia y hacer que la función de pérdida fluctúe alrededor del mínimo o incluso diverja.</p></li>
<li><p>Los horarios de actualización de la tasa de aprendizaje intentan ajustar la tasa de aprendizaje durante la entrenamiento, es decir, reducir la tasa de aprendizaje de acuerdo con un programa predefinido o cuando el cambio función de pérdida entre épocas cae por debajo de un umbral. Sin embargo, estos horarios y umbrales deben definirse con anticipación por lo que no pueden adaptarse a las características de un conjunto de datos.</p></li>
<li><p>Además, la misma tasa de aprendizaje se aplica a todas las actualizaciones de parámetros. Si nuestros datos son escasos y los valores de nuestras variables (características) tienen frecuencias muy diferentes, es posible que no queramos actualizarlas todas en la misma medida, sino realizar una actualización más grande para las características que ocurren con poca frecuencia.</p></li>
<li><p>Otro desafío clave al minimizar las funciones de error altamente no convexas comunes para las redes neuronales es evitar quedar atrapado en sus numerosos mínimos locales sub-óptimos. Algunos autores argumentan que, de hecho, la dificultad no surge de los mínimos locales sino de los puntos de silla, es decir, puntos donde una dimensión se inclina hacia arriba y otra hacia abajo. Estos puntos de silla suelen estar rodeados por una meseta del mismo error, lo que dificulta notablemente el escape de SGD, ya que el gradiente es cercano a cero en todas las dimensiones.</p></li>
</ol>
<p><img alt="silla" src="../Imagenes/Gradient_ascent_chair.png" /></p>
<p>Ejemplo de un punto de silla.</p>
<p>Fuente: <a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent">Wikipedia</a></p>
<p>Para una revisión contemporáneas de los algoritmos de optimización modernos puede consultar <a class="reference external" href="https://arxiv.org/pdf/1609.04747.pdf">An overview of gradient descent optimization
algorithms</a>.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent#/media/File:Gradient_Descent_Example_Nonlinear_Equations.gif">Visualización SGD en Wikipedia</a></p>
</div>
</div>
<div class="section" id="span-style-color-4361ee-metodo-del-momento-span">
<h2><span style="color:#4361EE">Método del momento</span><a class="headerlink" href="#span-style-color-4361ee-metodo-del-momento-span" title="Permalink to this headline">¶</a></h2>
<p>SGD tiene problemas para navegar por los barrancos, es decir, áreas donde la superficie se curva mucho más abruptamente en una dimensión que en otra, que son comunes en los óptimos locales. En estos escenarios, SGD oscila a lo largo de las pendientes del barranco mientras solo avanza vacilante por el fondo hacia el óptimo local.</p>
<p>El método del momento ayuda a acelerar SGD en la dirección relevante y amortigua oscilaciones. Lo hace sumando una fracción <span class="math notranslate nohighlight">\(\lambda\)</span> del vector de actualización del paso anterior al vector de actualización actual.</p>
<p>El método  se esquematiza como sigue</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathbf{v}_k &amp;= \lambda \mathbf{v}_{k-1} +  \eta \nabla_{\boldsymbol{\theta}} \mathfrak{L}({x}_{train}^{(i)},{y}_{train}^{(i)},\boldsymbol{\theta}_k)\\
\theta_{k+1} &amp;= \theta_{k} - v_k,
\end{align}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lambda&lt;1\)</span>. Usualmente, <span class="math notranslate nohighlight">\(\lambda= 0.9\)</span>.</p>
</div>
<div class="section" id="span-style-color-4361ee-rmsprop-span">
<h2><span style="color:#4361EE">RMSprop</span><a class="headerlink" href="#span-style-color-4361ee-rmsprop-span" title="Permalink to this headline">¶</a></h2>
<p>Desarrollado por Goeff Hinton, no publicado. Se basa en dividir la tasa de aprendizaje en cada caso por un promedio del cuadrado de las componentes del gradiente en el paso anterior. Por cada componente <span class="math notranslate nohighlight">\(\theta\)</span> del vector de parámetros <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, sea <span class="math notranslate nohighlight">\(g\)</span> la respectiva componente del gradiente asociada a <span class="math notranslate nohighlight">\(\theta\)</span>, entonces el métodos es como sigue:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(E[g^2]_t= \lambda E[g^2]_{t-1} + (1-\lambda)g_t^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_{t+1} = \theta_t - \tfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t\)</span></p></li>
</ol>
<p><span class="math notranslate nohighlight">\(\epsilon &gt;0\)</span> es para evitar divisiones por cero.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> es el parámetro de decaimiento. Típicamente <span class="math notranslate nohighlight">\(\lambda = 0.9\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\eta\)</span> es la tasa de aprendizaje. Típicamente el valor por defecto es 0.001.</p></li>
</ul>
</div>
<div class="section" id="span-style-color-4361ee-algoritmo-adam-span">
<h2><span style="color:#4361EE">Algoritmo Adam</span><a class="headerlink" href="#span-style-color-4361ee-algoritmo-adam-span" title="Permalink to this headline">¶</a></h2>
<p>El algoritmo <a class="reference external" href="https://arxiv.org/pdf/1412.6980.pdf">Adam a method for Stochastic optimization</a> de Kingma y Lei es actualmente el método más utilizado. El siguiente es el algoritmo.</p>
<p>El símbolo  <span class="math notranslate nohighlight">\(g^2_t\)</span> indica los elementos del producto de Hadamard (componente por componente)  <span class="math notranslate nohighlight">\(g_t\bigodot g_t\)</span>. Según los autores, los mejores resultados han sido obtenidos para los valores de los hiperparámetros  <span class="math notranslate nohighlight">\(\alpha = 0.001\)</span>, <span class="math notranslate nohighlight">\(\beta_1 = 0.9\)</span>, <span class="math notranslate nohighlight">\(\beta_2 = 0.999\)</span> y <span class="math notranslate nohighlight">\(\epsilon = 10−8\)</span>. Todas operaciones entre vectores son hechas componente por componente (producto de Hadamard). con <span class="math notranslate nohighlight">\(\beta_1^t\)</span> and <span class="math notranslate nohighlight">\(\beta_2^t\)</span> se denota la potencia <span class="math notranslate nohighlight">\(t\)</span>-ésima.</p>
<ol class="simple">
<li><p>Requerido: <span class="math notranslate nohighlight">\(\alpha\)</span>: Valor de salto (Stepsize)</p></li>
<li><p>Requerido: <span class="math notranslate nohighlight">\(\beta_1^t\)</span> y <span class="math notranslate nohighlight">\(\beta_2^t \in [0, 1)\)</span>. Ratas de decaimiento exponencial para la estimación de los momentos.</p></li>
<li><p>Requerido: <span class="math notranslate nohighlight">\(f(\theta)\)</span>: Función de pérdida objetivo con parámetros <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p>Requerido: <span class="math notranslate nohighlight">\(\theta_0\)</span>: Vector de valores iniciales del vector de parámetros.</p></li>
<li><p><span class="math notranslate nohighlight">\(m_0  = 0\)</span> (Inicialización del vector momento de primer orden).</p></li>
<li><p><span class="math notranslate nohighlight">\(v_0 =  0\)</span> (Inicialización del vector momento de segundo orden).</p></li>
<li><p><span class="math notranslate nohighlight">\(t =  0\)</span> (Inicialización del contador de iteraciones).</p></li>
<li><p>Mientras <span class="math notranslate nohighlight">\((||\theta_t - \theta_{t-1}||&gt;\delta\)</span>) (Mientras no haya covergencia repita los siguientes pasos:)</p></li>
</ol>
<p><span class="math notranslate nohighlight">\(
\begin{align}
t  &amp;= t + 1 \\
g_t &amp;=  \nabla f_t(\theta_{t-1}) \text{ (Obtenga los gradientes de la función de pérdida en el paso \)</span>t<span class="math notranslate nohighlight">\( con respeto a los parámetros)}\\
m_t  &amp;= \beta_1 m_{t−1} + (1 − \beta_1) · g_t \text{ (Actualice la estimación del vector de  momentos de primer orden)}\\
v_t  &amp;= \beta_2 v_{t−1} + (1 − \beta_2) · g_t^2 \text{ (Actualice la estimación del vector de  momentos de segundo orden)}\\
\hat{m}_t  &amp;= \frac{m_t}{1 − \beta_1^t}   \text{ (Calcule la corrección de sesgo de la estimación del vector de momentos de primer orden)}\\ 
\hat{v}_t  &amp;=  \frac{v_t}{1 − \beta_2^t}  \text{ (Calcule la corrección de sesgo de la estimación del vector de momentos de segundo orden)}\\ 
\theta_t &amp;=   \theta_{t-1}  - \alpha  \frac{\hat{m}_t}{\hat{v}_t + epsilon} \text{ (Actualice los parámetros)}\\
\end{align}
\)</span></p>
<p>fin mientras</p>
<p>return <span class="math notranslate nohighlight">\(\theta_t\)</span> (Parámetros resultantes)</p>
<p><img alt="Comparacion" src="../Imagenes/Adam_MsProp.png" /></p>
<p>Comparación de los diferentes métodos de gradiente descendiente estocástico</p>
<p>Fuente: Alvaro Montenegro</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "kamilo44/Prueba_1",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Intro_Tensores_II.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span style="color:#F72585"><center>Tensores</center></span></p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Prob_Conceptos_Basicos.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span style="color:#F72585"><center>Probabilidad</center></span></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      Por kamilo44<br/>
    
        &copy; Derechos de autor 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>